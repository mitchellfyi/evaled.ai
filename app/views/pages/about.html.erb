<% content_for :title, "About Evald.ai" %>
<% content_for :description, "Learn about Evald.ai - the trust score platform for AI agents. Built to solve the evaluation problem for AI agent adoption." %>

<div class="max-w-3xl mx-auto">
  <h1 class="text-4xl font-bold text-gray-900 mb-8">About Evald</h1>
  
  <div class="prose prose-lg text-gray-600">
    <p class="lead text-xl">
      Evald was born from a simple observation: we have no good way to evaluate AI agents before deploying them.
    </p>
    
    <h2 class="text-2xl font-bold text-gray-900 mt-12 mb-4">The Problem</h2>
    <p>
      You're evaluating an AI agent for your codebase, your customer data, or your production environment. 
      Your options are limited:
    </p>
    <ul class="list-disc pl-6 space-y-2">
      <li>Read the README and hope it's accurate</li>
      <li>Check GitHub stars (a popularity contest, not a quality signal)</li>
      <li>Ask on Twitter/Discord and get anecdotal answers</li>
      <li>Spend days running your own evaluations</li>
    </ul>
    
    <p class="text-xl font-medium text-gray-900">
      Protocols tell you how agents connect. Evald tells you whether you should.
    </p>
    <p>
      MCP tells you <strong>how</strong> agents connect. A2A tells you <strong>how</strong> agents talk to each other. 
      Nothing tells you <strong>whether you should trust them</strong>.
    </p>
    
    <h2 class="text-2xl font-bold text-gray-900 mt-12 mb-4">Our Solution</h2>
    <p>
      Evald continuously evaluates AI agents across multiple dimensions:
    </p>
    <ul class="list-disc pl-6 space-y-2">
      <li><strong>Passive signals</strong> from public repos (health, dependencies, documentation)</li>
      <li><strong>Task completion</strong> against standardized benchmarks</li>
      <li><strong>Safety behavior</strong> and scope discipline</li>
      <li><strong>Real-world performance</strong> from production telemetry (coming soon)</li>
    </ul>
    
    <p>
      Every agent gets a public trust profile with an Evald Score (0-100) that decays over time, 
      so you always know how fresh the evaluation is.
    </p>
    
    <h2 class="text-2xl font-bold text-gray-900 mt-12 mb-4">Origin</h2>
    <p>
      Evald was originally hypothesized in 
      <a href="https://www.mitchellbryson.com/articles/the-trust-stack-ai-agents" class="text-indigo-600 hover:text-indigo-700">
        "The Trust Stack: Identity, Reputation, and Accountability for AI Agents"
      </a>, 
      an essay exploring the infrastructure needed for agents to operate in the economy.
    </p>
    
    <h2 class="text-2xl font-bold text-gray-900 mt-12 mb-4">Built By</h2>
    <p>
      Evald is built by <a href="https://mitchellbryson.com" class="text-indigo-600 hover:text-indigo-700">Mitchell Bryson</a>, 
      a product engineer focused on AI systems and agentic infrastructure.
    </p>
    
    <p>
      The project is open source: 
      <a href="https://github.com/mitchellfyi/evald.ai" class="text-indigo-600 hover:text-indigo-700">github.com/mitchellfyi/evald.ai</a>
    </p>
  </div>
</div>
